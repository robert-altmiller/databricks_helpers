# Loyalty 2.0 DAB Framework - Architecture Document

**Version:** 1.0  
**Date:** December 10, 2025  
**Status:** Production Ready

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Component Architecture](#component-architecture)
4. [Data Flow Architecture](#data-flow-architecture)
5. [Deployment Architecture](#deployment-architecture)
6. [Pipeline Types](#pipeline-types)
7. [Configuration Architecture](#configuration-architecture)
8. [Security Architecture](#security-architecture)

---

## ğŸ¯ Overview

### Purpose
Production-ready Databricks Asset Bundle (DAB) framework for deploying and managing data pipelines in the Loyalty 2.0 platform.

### Key Features
- âœ… **Multi-Pipeline Support**: Streaming and Batch SQL pipelines
- âœ… **Auto-Detection**: Automatic pipeline type identification
- âœ… **Multi-Environment**: Dev, Staging, Production isolation
- âœ… **Clean Naming**: `[Loyalty2.0]` branded job names
- âœ… **One-Command Deploy**: Simple deployment workflow

---

## ğŸ—ï¸ System Architecture

### High-Level System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LOYALTY 2.0 DAB FRAMEWORK                        â”‚
â”‚                     (Production Environment)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                          â”‚                          â”‚
        â–¼                          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Developer   â”‚        â”‚  CI/CD System â”‚        â”‚  Data Ops     â”‚
â”‚   Workstation â”‚        â”‚   (Optional)  â”‚        â”‚     Team      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                        â”‚
        â”‚                        â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Deploy Script        â”‚
                    â”‚   ./deploy.sh          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ [Auto-Detect Pipeline Type]
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                 â”‚
            â–¼                                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Streaming Gen   â”‚              â”‚  Batch SQL Gen  â”‚
   â”‚ generate_dab_   â”‚              â”‚  generate_dab   â”‚
   â”‚ streaming.py    â”‚              â”‚  .py            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ databricks.yml  â”‚
                    â”‚  (Generated)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Databricks Workspace    â”‚
              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
              â”‚  â”‚  Bundle Validate   â”‚  â”‚
              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
              â”‚             â”‚             â”‚
              â”‚             â–¼             â”‚
              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
              â”‚  â”‚  Bundle Deploy     â”‚  â”‚
              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
              â”‚             â”‚             â”‚
              â”‚             â–¼             â”‚
              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
              â”‚  â”‚  Jobs Created      â”‚  â”‚
              â”‚  â”‚  & Ready to Run    â”‚  â”‚
              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Component Architecture

### Framework Components

```
loyalty2.0_dab_framework/
â”‚
â”œâ”€â”€â”€ ğŸ“‚ config/                    â† Environment Configuration Layer
â”‚    â”œâ”€â”€ dev.yml                   (2 workers, 30min timeout)
â”‚    â”œâ”€â”€ staging.yml               (3 workers, 60min timeout)
â”‚    â””â”€â”€ prod.yml                  (5 workers, monitoring enabled)
â”‚
â”œâ”€â”€â”€ ğŸ“‚ pipelines/                 â† Data Pipeline Layer
â”‚    â”œâ”€â”€ kafka_to_lakebase/        [Streaming â†’ Postgres]
â”‚    â”œâ”€â”€ gap_injection_with_kafka/ [Streaming â†’ Delta Bronze]
â”‚    â”œâ”€â”€ kafka_traffic_pipeline/   [Batch: Bronze â†’ Silver â†’ Gold]
â”‚    â””â”€â”€ template/                 [Template for new pipelines]
â”‚
â”œâ”€â”€â”€ ğŸ“‚ src/utils/                 â† Code Generation Layer
â”‚    â”œâ”€â”€ generate_dab.py           (Batch SQL pipelines)
â”‚    â”œâ”€â”€ generate_dab_streaming.py (Streaming pipelines)
â”‚    â””â”€â”€ logger.py                 (Logging utility)
â”‚
â”œâ”€â”€â”€ ğŸ“‚ scripts/                   â† Orchestration Layer
â”‚    â”œâ”€â”€ deploy.sh                 (Universal deployment)
â”‚    â””â”€â”€ setup.sh                  (Initial setup)
â”‚
â”œâ”€â”€â”€ ğŸ“‚ tests/                     â† Validation Layer
â”‚    â””â”€â”€ test_deployment.py        (10 tests: structure + generation)
â”‚
â””â”€â”€â”€ ğŸ“„ databricks.yml             â† Root Bundle Definition
```

---

## ğŸ“Š Data Flow Architecture

### Complete Data Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸŒŠ Kafka Topics              ğŸ“ External Systems                   â”‚
â”‚  â€¢ jomin_johny_fe_tech...     â€¢ APIs                                â”‚
â”‚  â€¢ Real-time events           â€¢ Databases                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                             â”‚
            â”‚                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BRONZE LAYER (Raw Data)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Catalog: loyalty_dev                                               â”‚
â”‚  Schema: kafka_bronze                                               â”‚
â”‚  Tables:                                                            â”‚
â”‚    â€¢ kafka_data_bronze                                              â”‚
â”‚      - Raw Kafka messages                                           â”‚
â”‚      - With metadata (topic, partition, offset, timestamp)          â”‚
â”‚                                                                     â”‚
â”‚  Pipeline: Kafka Ingestion to Bronze                                â”‚
â”‚    â””â”€ Job: kafka_ingestion_to_bronze_pipeline                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ [ETL Transformation]
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SILVER LAYER (Cleaned Data)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Catalog: loyalty_dev                                               â”‚
â”‚  Schema: silver_db                                                  â”‚
â”‚  Tables:                                                            â”‚
â”‚    â€¢ traffic_data_cleaned                                           â”‚
â”‚      - Cleaned & validated                                          â”‚
â”‚      - Data quality scores                                          â”‚
â”‚      - Composite keys                                               â”‚
â”‚    â€¢ customer_engagement_summary                                    â”‚
â”‚      - Aggregated metrics                                           â”‚
â”‚      - Customer segmentation                                        â”‚
â”‚                                                                     â”‚
â”‚  Pipeline: Traffic Data ETL Pipeline                                â”‚
â”‚    â””â”€ Job: traffic_data_etl_pipeline                                â”‚
â”‚       â”œâ”€ Step 0: setup_databases                                    â”‚
â”‚       â”œâ”€ Step 1: traffic_data_clean                                 â”‚
â”‚       â””â”€ Step 2: user_engagement_summary                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ [Business Logic]
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GOLD LAYER (Business Data)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Catalog: loyalty_dev                                               â”‚
â”‚  Schema: gold_db                                                    â”‚
â”‚  Tables:                                                            â”‚
â”‚    â€¢ traffic_data_enriched                                          â”‚
â”‚      - Business-ready data                                          â”‚
â”‚      - No technical metadata                                        â”‚
â”‚      - Optimized for analytics                                      â”‚
â”‚    â€¢ customer_locations                                             â”‚
â”‚      - Dimension table                                              â”‚
â”‚                                                                     â”‚
â”‚  Pipeline: Traffic Data ETL Pipeline                                â”‚
â”‚    â””â”€ Job: traffic_data_etl_pipeline                                â”‚
â”‚       â””â”€ Step 3: traffic_data_enriched                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          EXTERNAL TARGETS (Parallel Stream)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ˜ PostgreSQL (Lakebase)                                           â”‚
â”‚    â€¢ Instance: jomin-gap-demo                                       â”‚
â”‚    â€¢ Database: databricks_postgres                                  â”‚
â”‚    â€¢ Schema: gold_db                                                â”‚
â”‚    â€¢ Table: kafka_data                                              â”‚
â”‚                                                                     â”‚
â”‚  Pipeline: Kafka to Lakebase Streaming                              â”‚
â”‚    â””â”€ Job: kafka_to_lakebase_streaming_pipeline                     â”‚
â”‚       â”œâ”€ Task 1: kafka_to_postgres_ingestion                        â”‚
â”‚       â””â”€ Task 2: customer_stats_generator                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Deployment Architecture

### Deployment Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEPLOYMENT WORKFLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Developer Action
        â”‚
        â”œâ”€ ./deploy.sh <pipeline_name> <env> <profile>
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 1: Pipeline Type Detection           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  â€¢ Read config.json                        â”‚
   â”‚  â€¢ Check for 'tasks' â†’ Streaming           â”‚
   â”‚  â€¢ Check for 'execution_sequence' â†’ Batch  â”‚
   â”‚  â€¢ Auto-detect and proceed                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 2: Generate databricks.yml           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  IF Streaming:                             â”‚
   â”‚    â†’ generate_dab_streaming.py             â”‚
   â”‚    â†’ Parse tasks, build job config         â”‚
   â”‚    â†’ Set timeout=0 (continuous)            â”‚
   â”‚                                            â”‚
   â”‚  IF Batch:                                 â”‚
   â”‚    â†’ generate_dab.py                       â”‚
   â”‚    â†’ Parse execution_sequence              â”‚
   â”‚    â†’ Build task dependencies               â”‚
   â”‚    â†’ Set timeout>0 (batch)                 â”‚
   â”‚                                            â”‚
   â”‚  OUTPUT: databricks.yml                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 3: Validate Bundle                   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  â€¢ databricks bundle validate              â”‚
   â”‚  â€¢ Check YAML syntax                       â”‚
   â”‚  â€¢ Verify workspace connection             â”‚
   â”‚  â€¢ Validate resource definitions           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 4: Deploy to Databricks              â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  â€¢ Upload files to workspace               â”‚
   â”‚  â€¢ Create/update jobs                      â”‚
   â”‚  â€¢ Configure clusters                      â”‚
   â”‚  â€¢ Set up notifications                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  STEP 5: Deployment Summary                â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  â€¢ Show job URL                            â”‚
   â”‚  â€¢ Display job configuration               â”‚
   â”‚  â€¢ Provide run command                     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
              âœ… COMPLETE
         Job ready in Databricks!
```

---

## ğŸ”„ Pipeline Types

### 1. Streaming Pipelines

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             STREAMING PIPELINE ARCHITECTURE             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Config Marker: "tasks" key present

  Kafka Topic
      â”‚
      â”œâ”€ Read Stream (Continuous)
      â”‚
      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Task 1:        â”‚
  â”‚  Ingestion      â”‚â—„â”€â”€â”€ Shared Job Cluster
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     (Cost Efficient)
           â”‚
           â”‚ [Parallel]
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Task 2:        â”‚â—„â”€â”€â”€ Same Cluster
  â”‚  Processing     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    Target System
    (Delta/Postgres)

Characteristics:
â€¢ timeout_seconds: 0 (continuous)
â€¢ Shared cluster for all tasks
â€¢ Real-time processing
â€¢ Checkpoint-based recovery

Examples:
â€¢ Kafka to Lakebase Streaming
â€¢ Kafka Ingestion to Bronze
```

### 2. Batch SQL Pipelines

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BATCH SQL PIPELINE ARCHITECTURE            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Config Marker: "execution_sequence" key present

  Step 0: Setup
      â”‚
      â””â”€ Create databases & schemas
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Bronze â†’ Silver   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Step 1 & Step 2   â”‚â—„â”€â”€â”€ Serverless/Dedicated
  â”‚  (Parallel)        â”‚     Compute
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ [depends_on]
             â”‚
             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Silver â†’ Gold     â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Step 3            â”‚â—„â”€â”€â”€ Task Dependencies
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
      Gold Layer Tables

Characteristics:
â€¢ timeout_seconds: 10800 (3 hours)
â€¢ Task dependencies (DAG)
â€¢ Scheduled execution
â€¢ Bronze â†’ Silver â†’ Gold

Examples:
â€¢ Traffic Data ETL Pipeline
```

---

## âš™ï¸ Configuration Architecture

### Configuration Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONFIGURATION ARCHITECTURE                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ROOT: databricks.yml
â”‚
â”œâ”€ Bundle Metadata
â”‚  â””â”€ name: loyalty2.0_framework
â”‚
â”œâ”€ Global Variables
â”‚  â”œâ”€ catalog (default: loyalty_dev)
â”‚  â””â”€ notification_email
â”‚
â”œâ”€ Environment Targets
â”‚  â”œâ”€ dev/
â”‚  â”‚  â”œâ”€ mode: development
â”‚  â”‚  â”œâ”€ workspace: {...}
â”‚  â”‚  â””â”€ variables: {...}
â”‚  â”‚
â”‚  â”œâ”€ staging/
â”‚  â”‚  â”œâ”€ mode: development
â”‚  â”‚  â””â”€ variables: {...}
â”‚  â”‚
â”‚  â””â”€ prod/
â”‚     â”œâ”€ mode: production
â”‚     â””â”€ variables: {...}
â”‚
â””â”€ Includes
   â””â”€ pipelines/*/databricks.yml

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PIPELINE LEVEL: pipelines/<name>/config.json
â”‚
â”œâ”€ Pipeline Identity
â”‚  â”œâ”€ pipeline_name: "Kafka to Lakebase Streaming"
â”‚  â”œâ”€ pipeline_type: "streaming_to_postgres"
â”‚  â””â”€ description: "..."
â”‚
â”œâ”€ Pipeline Tasks/Steps
â”‚  â”œâ”€ tasks: [...]              (for streaming)
â”‚  â””â”€ execution_sequence: [...]  (for batch)
â”‚
â”œâ”€ Resource Configuration
â”‚  â”œâ”€ job_cluster: {...}
â”‚  â”œâ”€ kafka_source: {...}
â”‚  â””â”€ postgres_connection: {...}
â”‚
â””â”€ Settings
   â”œâ”€ notification_email
   â””â”€ max_concurrent_runs

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ENVIRONMENT: config/<env>.yml
â”‚
â”œâ”€ Environment Identity
â”‚  â””â”€ environment: dev/staging/prod
â”‚
â”œâ”€ Databricks Config
â”‚  â”œâ”€ host
â”‚  â””â”€ workspace_path
â”‚
â”œâ”€ Catalog Config
â”‚  â”œâ”€ name
â”‚  â””â”€ schemas: [bronze, silver, gold]
â”‚
â”œâ”€ Cluster Defaults
â”‚  â”œâ”€ spark_version
â”‚  â”œâ”€ node_type_id
â”‚  â”œâ”€ num_workers
â”‚  â””â”€ autotermination_minutes
â”‚
â””â”€ Pipeline Defaults
   â”œâ”€ timeout_seconds
   â”œâ”€ checkpoint_location
   â””â”€ max_concurrent_runs
```

---

## ğŸ”’ Security Architecture

### Security Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SECURITY ARCHITECTURE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Layer 1: Authentication
â”œâ”€ Databricks CLI
â”‚  â”œâ”€ Profile-based auth (--profile jomin)
â”‚  â”œâ”€ Token-based authentication
â”‚  â””â”€ OAuth support (future)
â”‚
â””â”€ Workspace Access
   â”œâ”€ User: jomin.johny@databricks.com
   â””â”€ Permissions: CAN_MANAGE

Layer 2: Authorization
â”œâ”€ Unity Catalog
â”‚  â”œâ”€ Catalog-level permissions
â”‚  â”œâ”€ Schema-level access control
â”‚  â””â”€ Table-level grants
â”‚
â””â”€ Job Permissions
   â”œâ”€ Group: loyalty-viewers (CAN_VIEW)
   â””â”€ Group: loyalty-operators (CAN_MANAGE_RUN)

Layer 3: Data Security
â”œâ”€ Secrets Management
â”‚  â”œâ”€ Secret Scopes
â”‚  â”‚  â””â”€ oetrta (Kafka credentials)
â”‚  â”œâ”€ Secret Keys
â”‚  â”‚  â””â”€ kafka-bootstrap-servers-plaintext
â”‚  â””â”€ No hardcoded credentials
â”‚
â””â”€ Data Isolation
   â”œâ”€ Dev: loyalty_dev catalog
   â”œâ”€ Staging: loyalty_staging catalog
   â””â”€ Prod: loyalty_prod catalog

Layer 4: Compute Security
â”œâ”€ Cluster Security Mode
â”‚  â””â”€ USER_ISOLATION (enforced)
â”‚
â”œâ”€ Network Security
â”‚  â””â”€ Private subnet communication
â”‚
â””â”€ Data at Rest
   â””â”€ Encrypted Delta tables

Layer 5: Audit & Monitoring
â”œâ”€ Job Notifications
â”‚  â”œâ”€ on_failure: email alerts
â”‚  â””â”€ on_success: prod only
â”‚
â”œâ”€ Workspace Logs
â”‚  â””â”€ All deployment actions logged
â”‚
â””â”€ Data Lineage
   â””â”€ Unity Catalog tracking
```

---

## ğŸ“ˆ Naming Convention

### Job Naming Pattern

```
Job Name Format:
[Loyalty2.0] [${bundle.target}] <Pipeline Display Name>

Examples:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Environment â”‚ Pipeline Name                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dev         â”‚ [Loyalty2.0] [dev] Kafka to Lakebase  â”‚
â”‚             â”‚ Streaming                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ staging     â”‚ [Loyalty2.0] [staging] Traffic Data   â”‚
â”‚             â”‚ ETL Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ prod        â”‚ [Loyalty2.0] [prod] Kafka Ingestion   â”‚
â”‚             â”‚ to Bronze                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Brand visibility ([Loyalty2.0])
âœ… Environment clarity ([dev/staging/prod])
âœ… Human-readable names (Kafka to Lakebase Streaming)
```

---

## ğŸ”„ End-to-End Workflow

### Complete Pipeline Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            PIPELINE LIFECYCLE WORKFLOW                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DEVELOPMENT
   â”‚
   â”œâ”€ Create pipeline from template
   â”‚  â””â”€ cp -r pipelines/template pipelines/my_pipeline
   â”‚
   â”œâ”€ Configure pipeline
   â”‚  â””â”€ Edit config.json (pipeline_name, tasks, settings)
   â”‚
   â””â”€ Add notebooks
      â””â”€ Write transformation logic

2. TESTING
   â”‚
   â”œâ”€ Run unit tests
   â”‚  â””â”€ python3 tests/test_deployment.py
   â”‚
   â”œâ”€ Generate bundle locally
   â”‚  â””â”€ python3 src/utils/generate_dab_streaming.py my_pipeline
   â”‚
   â””â”€ Validate YAML
      â””â”€ databricks bundle validate -t dev

3. DEPLOYMENT (DEV)
   â”‚
   â”œâ”€ Deploy to dev
   â”‚  â””â”€ ./scripts/deploy.sh my_pipeline dev jomin
   â”‚
   â”œâ”€ Run pipeline
   â”‚  â””â”€ databricks bundle run <job> -t dev --profile jomin
   â”‚
   â””â”€ Verify results
      â””â”€ Check tables, logs, metrics

4. PROMOTION (STAGING)
   â”‚
   â”œâ”€ Deploy to staging
   â”‚  â””â”€ ./scripts/deploy.sh my_pipeline staging jomin
   â”‚
   â”œâ”€ Run integration tests
   â”‚  â””â”€ Verify with real data volumes
   â”‚
   â””â”€ Performance validation
      â””â”€ Check cluster usage, execution time

5. PRODUCTION (PROD)
   â”‚
   â”œâ”€ Deploy to prod
   â”‚  â””â”€ ./scripts/deploy.sh my_pipeline prod jomin
   â”‚
   â”œâ”€ Schedule job
   â”‚  â””â”€ Configure cron expression
   â”‚
   â”œâ”€ Monitor
   â”‚  â””â”€ Email notifications, dashboards
   â”‚
   â””â”€ Maintain
      â””â”€ Update notebooks, redeploy as needed
```

---

## ğŸ“Š Metrics & Monitoring

### Key Metrics

```
Pipeline Health Metrics:
â”œâ”€ Execution Time
â”‚  â”œâ”€ Avg: < 30 minutes (batch)
â”‚  â””â”€ Continuous (streaming)
â”‚
â”œâ”€ Success Rate
â”‚  â””â”€ Target: > 99%
â”‚
â”œâ”€ Data Quality
â”‚  â”œâ”€ Null percentage
â”‚  â”œâ”€ Duplicate count
â”‚  â””â”€ Schema compliance
â”‚
â””â”€ Resource Utilization
   â”œâ”€ Cluster efficiency
   â””â”€ Cost per pipeline run

Framework Metrics:
â”œâ”€ Deployment Success Rate
â”‚  â””â”€ Target: 100%
â”‚
â”œâ”€ Validation Pass Rate
â”‚  â””â”€ 10/10 tests passing
â”‚
â””â”€ Time to Deploy
   â””â”€ Avg: < 2 minutes
```

---

## ğŸ¯ Summary

### Framework Capabilities

| Capability | Status | Details |
|-----------|--------|---------|
| **Multi-Pipeline** | âœ… Production | 3 pipelines deployed |
| **Auto-Detection** | âœ… Production | Streaming & Batch SQL |
| **Multi-Environment** | âœ… Production | Dev, Staging, Prod |
| **Clean Naming** | âœ… Production | `[Loyalty2.0]` branded |
| **Testing** | âœ… Production | 10/10 tests passing |
| **Documentation** | âœ… Complete | This document + guides |

### Deployment Stats

- **Total Pipelines**: 3
- **Total Jobs**: 3 (one per pipeline)
- **Total Tasks**: 7 (across all pipelines)
- **Lines of Code**: 2,559
- **Python Files**: 17
- **Tests Passing**: 10/10 (100%)

---

## ğŸ“ Support

For questions or issues:
- **Documentation**: README.md, DEPLOYMENT_GUIDE.md
- **Tests**: `python3 tests/test_deployment.py`
- **Contact**: data-team@company.com

---

**Built with â¤ï¸ following Databricks & Microsoft best practices**

*End of Architecture Document*

