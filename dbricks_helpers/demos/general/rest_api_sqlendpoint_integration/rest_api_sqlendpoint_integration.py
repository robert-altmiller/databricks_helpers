# Databricks notebook source
import requests
import os
import json 
#from dotenv import load_dotenv
import time
import timeit
import glob

#load_dotenv()

CLIENT_ID = 'xxx'
CLIENT_SECRET = 'xxxxxxxx'
TENANT_ID = 'xxxxxxxxx'
DATABRICKS_RESOURCE_ID = '2ff814a6-3304-4ab8-85cb-cd0e6f879c1d'
DATABRICKS_WORKSPACE = 'https://adb-4247081124391553.13.azuredatabricks.net/'
TOKEN_REQ_BODY = {'grant_type': 'client_credentials',
                  'client_id':  CLIENT_ID,
                  'client_secret':  CLIENT_SECRET,
                  'resource': DATABRICKS_RESOURCE_ID}
TOKEN_BASE_URL = 'https://login.microsoftonline.com/' + TENANT_ID + '/oauth2/token'
TOKEN_REQ_HEADERS = {'Content-Type': 'application/x-www-form-urlencoded'}

DATABRICKS_STATEMENT_ENDPOINT = 'api/2.0/sql/statements/'

def remove_old_result_files():
    """Remove files from directory data"""
    file_path = glob.glob("data\\*")
    for file in file_path:
        os.remove(file)


def generate_bearer_token():
    """Get a Azure AD Token"""
    response = requests.get(TOKEN_BASE_URL, headers=TOKEN_REQ_HEADERS, data=TOKEN_REQ_BODY)
    if response.status_code == 200:
        print(response.status_code)
    else:
        raise Exception(response.text)
    return response.json()['access_token']


def execute_query_databricks(query_json, token_bearer):
    """Execute a SQL Statement on Databricks and wait for completion"""
    final_status = ['SUCCEEDED', 'FAILED']
    databricks_req_headers = {'Authorization': 'Bearer ' + token_bearer}
    response = requests.post(DATABRICKS_WORKSPACE + '/' + DATABRICKS_STATEMENT_ENDPOINT,
                             headers = databricks_req_headers,
                             json = json.loads(query_json)).json()
    sql_statement_id = response['statement_id']
    while (response['status']['state'] not in final_status):
        response = requests.get(DATABRICKS_WORKSPACE  + '/api/2.0/sql/statements/' + sql_statement_id + '/',
                                  headers = databricks_req_headers).json()
        time.sleep(5)
    return response


def download_results(response_query, token_bearer):
    """Download the result set from each chunk generated by SQL statement"""
    sql_statement_id = response_query['statement_id']
    total_chunk_count = response_query['manifest']['total_chunk_count']
    databricks_req_headers = {'Authorization': 'Bearer ' + token_bearer}
    if total_chunk_count == 1:
        external_link_chunk = response_query['result']['external_links'][0]['external_link']
        request_file = requests.get(external_link_chunk)
        open('data\\result_query_0', 'wb').write(request_file.content)
    else:        
        for chunk in range(0, total_chunk_count):
            response = requests.get(DATABRICKS_WORKSPACE  + '/api/2.0/sql/statements/' + sql_statement_id + '/result/chunks/' + str(chunk),
                                    headers = databricks_req_headers).json()
            external_link_chunk = response['external_links'][0]['external_link']
            request_file = requests.get(external_link_chunk)
            print('Downloading chunk ' + str(chunk) + '...')
            open('data\\result_query_' + str(chunk), 'wb').write(request_file.content)


if __name__ == "__main__":    
    warehouse_id = "12f993f23976ce13"
    catalog = "samples"
    schema = "tpch"
    statement = "SELECT * FROM samples.tpch.orders limit 10"
    parameters = "[]"
    query_json = '{"warehouse_id": "' + warehouse_id + '", ' + \
                 '"catalog": "' + catalog + '", ' + \
                 '"schema": "' + schema + '", ' + \
                 '"disposition": "EXTERNAL_LINKS", ' + \
                 '"statement": "' + statement + '", ' + \
                 '"parameters": ' + parameters + '}'

    remove_old_result_files()
    token_bearer = generate_bearer_token()
    response = execute_query_databricks(query_json, token_bearer)
    print(response)
    download_results(response, token_bearer)